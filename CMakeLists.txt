cmake_minimum_required(VERSION 3.24)
project(llm.c LANGUAGES C)
# project(llm.c LANGUAGES C CXX CUDA)

# Put binaries and libraries in the same location.
set(CMAKE_ARCHIVE_OUTPUT_DIRECTORY ${PROJECT_BINARY_DIR}/lib)
set(CMAKE_LIBRARY_OUTPUT_DIRECTORY ${PROJECT_BINARY_DIR}/lib)
set(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${PROJECT_BINARY_DIR}/bin)

set(CMAKE_POSITION_INDEPENDENT_CODE ON)

# Always export compile_commands.json for lsp like clangd.
set(CMAKE_EXPORT_COMPILE_COMMANDS ON)

# We don't support this compiler.
if (CMAKE_CXX_COMPILER_ID STREQUAL "MSVC")
  message(FATAL_ERROR "This compiler is not supported")
endif()

# Release by default if not specified.
if (NOT EXISTS ${CMAKE_BINARY_DIR}/CMakeCache.txt)
  if (NOT CMAKE_BUILD_TYPE)
    set(CMAKE_BUILD_TYPE "Release" CACHE STRING "" FORCE)
  endif()
endif()

# option(PRECISION "Precision settings" BF16)
# option(USE_CUDNN "Use cudnn" ON)

add_library(train_gpt2_cpu train_gpt2.c)
target_include_directories(train_gpt2_cpu PUBLIC ${CMAKE_CURRENT_SOURCE_DIR} ${PROJECT_SOURCE_DIR}/llmc)
target_link_libraries(train_gpt2_cpu PRIVATE m)
target_compile_definitions(train_gpt2_cpu PRIVATE -DLLMC_LIB=1)
if (NO_OMP)
  message(STATUS "OpenMP is manually disabled")
else()
  find_package(OpenMP)
  if (OpenMP_FOUND)
    message(STATUS "✓ OpenMP found")
    target_link_libraries(train_gpt2_cpu PRIVATE OpenMP::OpenMP_C)
  else()
    message(STATUS "✗ OpenMP not found")
  endif()
endif()
target_compile_options(train_gpt2_cpu PRIVATE -Ofast -Wno-unused-result -Wno-ignored-pragmas -Wno-unknown-attributes -march=native)

# set_source_files_properties(llmc/cudnn_att.cpp PROPERTIES LANGUAGE CUDA)
# add_library(train_gpt2_cuda SHARED train_gpt2.cu llmc/cudnn_att.cpp)
# target_include_directories(train_gpt2_cuda PUBLIC ${CMAKE_CURRENT_SOURCE_DIR} ${PROJECT_SOURCE_DIR}/llmc)
# target_compile_options(train_gpt2_cuda PRIVATE -O3 -t=0 --use_fast_math)
# set_target_properties(train_gpt2_cuda PROPERTIES CXX_STANDARD 17)
# if (PRECISION EQUAL "FP32")
#   target_compile_definitions(train_gpt2_cuda PRIVATE -DENABLE_FP32)
# elseif(PRECISION EQUAL "FP16")
#   target_compile_definitions(train_gpt2_cuda PRIVATE -DENABLE_FP16)
# else()
#   target_compile_definitions(train_gpt2_cuda PRIVATE -DENABLE_BF16)
# endif()


# Disable cudnn for now, it has soem bugs in its cmake.
# if (USE_CUDNN)
#   include(FetchContent)
#   FetchContent_Declare(cudnn-frontend URL https://github.com/NVIDIA/cudnn-frontend/archive/refs/tags/v1.5.2.tar.gz)
#   FetchContent_MakeAvailable(cudnn-frontend)
#   target_compile_definitions(train_gpt2_cuda PRIVATE -DENABLE_CUDNN)
#   target_link_libraries(train_gpt2_cuda PRIVATE cudnn)
# endif()

# if (NO_USE_MPI)
#   message(STATUS "→ MPI is manually disabled")
# else()
#   find_package(MPI)
#   if (MPI_FOUND)
#     message(STATUS "✓ MPI found")
#     target_compile_definitions(train_gpt2_cuda PRIVATE -DUSE_MPI)
#     target_link_libraries(train_gpt2_cuda PRIVATE MPI::MPI_C)
#   else()
#       message(STATUS "✗ MPI not found")
#   endif()
# endif()
#
# if (NO_MULTI_GPU)
#   message(STATUS "→ Multi-GPU (NCCL) is manually disabled")
# else()
#   find_package(NCCL)
#   if (NCCL_FOUND)
#     message(STATUS "✓ NCCL found, OK to train with multiple GPUs")
#     target_compile_definitions(train_gpt2_cuda PRIVATE -DMULTI_GPU)
#     target_link_libraries(train_gpt2_cuda PRIVATE NCCL::NCCL_C)
#   else()
#     message(STATUS "✗ NCCL is not found, disabling multi-GPU support")
#   endif()
# endif()
